{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxi/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import check_random_state\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist(train_size, test_size):\n",
    "    mnist = fetch_mldata('MNIST original')\n",
    "    X = mnist.data.astype('float64')\n",
    "    y = mnist.target\n",
    "    random_state = check_random_state(0)\n",
    "\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, \n",
    "                        train_size=train_size, test_size=test_size, random_state= random_state)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    x_train = scaler.fit_transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "#expected = y_test\n",
    "#predicted = y_test #Network output\n",
    "\n",
    "#print(\"Classification report:\")\n",
    "#print(classification_report(expected, predicted))\n",
    "#print(\"Confusion matrix:\")\n",
    "#print(confusion_matrix(expected, predicted))\n",
    "\n",
    "def get_logs_path(session):\n",
    "    \"\"\" Returns path where log is going to be saved.\n",
    "    @Input session is the parent folder for the runs\n",
    "    So e.g. (Session1 -> run_00, run_01, run_02 ),\n",
    "    (Session2 -> run_00, run_01)\"\"\"\n",
    "    curr_dir = os.getcwd()\n",
    "    logs_path = curr_dir + '/tensorflow_logs/train_mnist/' + session\n",
    "    if not os.path.exists(logs_path):\n",
    "        os.makedirs(logs_path)\n",
    "    previous_runs = os.listdir(logs_path)\n",
    "    new_path = logs_path + \"/run_{:02d}\".format(len(previous_runs))\n",
    "    if not os.path.exists(new_path):\n",
    "        os.makedirs(new_path)\n",
    "        print(\"Saving log at: \", new_path)\n",
    "        return new_path\n",
    "    raise Exception(\"Path is already existing: \", new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Ziel ist eine nn struktur die sich durch tweaken von eins zwei parametern komplett verändern lässt\n",
    "dabei sollte die depth variable sein, so wie wie viele hidden units ich habe\n",
    "wie die variablen aufgesetzt werden sollte auch per input regulierbar sein\n",
    "Maybe with dropout etc.\"\"\"\n",
    "\n",
    "def forward(input_samples, num_hidden=2, output_units=[20,10,10,10]):\n",
    "    \"\"\" this is the whole network structure, returns the last layer after computing\n",
    "    every other layer\"\"\"\n",
    "    \n",
    "    #assert that inputs are vaguely corretct:\n",
    "    assert len(output_units) == num_hidden + 2,\\\n",
    "    \"output_units length: {}, while only {} num_hidden\".format(len(output_units),  num_hidden)\n",
    "    \n",
    "    #reshape input data\n",
    "    input_layer = input_samples\n",
    "    \n",
    "    #input layer\n",
    "    first_layer = tf.layers.dense(inputs=input_layer, units=output_units[0], name=\"Input_layer\")\n",
    "    \n",
    "    layers = {\n",
    "        \"l0\": first_layer \n",
    "    }\n",
    "    \n",
    "    #some deep layers num_hidden iterations, just shifted one forward\n",
    "    for i in range(1,num_hidden+1):\n",
    "        prev_layer = layers[\"l\"+str(i-1)]\n",
    "        layers[\"l\"+str(i)] = tf.layers.dense(inputs=prev_layer, \n",
    "                                             units=output_units[i], name=\"Hidden_layer_\"+str(i))\n",
    "        \n",
    "    \n",
    "    #output layer\n",
    "    out_layer = tf.layers.dense(inputs=layers[\"l\"+str(num_hidden)], units=output_units[-1], name=\"Output_Layer\")\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x_train, y_train, batch_size, input_size, num_epochs, writer):\n",
    "    \"\"\" trains the network defined in forward \"\"\"\n",
    "    \n",
    "    x_ph = tf.placeholder(tf.float32, [batch_size, input_size], name=\"x_ph\")\n",
    "    y_ph = tf.placeholder(tf.int32, [batch_size], name=\"y_ph\")\n",
    "    \n",
    "    print(\"x placeholder: \", x_ph)\n",
    "    prediction = forward(x_ph)\n",
    "    \n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels= y_ph, logits=prediction)\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    train_op = optimizer.minimize(loss)\n",
    "    \n",
    "    tf.summary.scalar(\"loss\",loss)\n",
    "    summ = tf.summary.merge_all()\n",
    "    \n",
    "    batches_per_epoch = x_train.shape[0] // batch_size\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            for batch in range(batches_per_epoch):\n",
    "                x_batch = x_train[batch * batch_size : (batch+1) * batch_size]\n",
    "                y_batch = y_train[batch * batch_size : (batch+1) * batch_size]\n",
    "                \n",
    "                _, cur_loss, cur_summ = sess.run([train_op, loss, summ], feed_dict={x_ph: x_batch, y_ph: y_batch})\n",
    "                writer.add_summary(cur_summ, epoch*batches_per_epoch + batch)\n",
    "                print(cur_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model():\n",
    "    x_train, y_train, x_test, y_test = get_mnist(1000,300)\n",
    "    logs_path = get_logs_path(\"Session_00\")\n",
    "    writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "    train(x_train=x_train, y_train=y_train, batch_size=100, input_size=784, num_epochs=5, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving log at:  /media/maxi/88EE5786EE576C06/code/github/trial_and_error/Projekt_Deep_arch/tensorflow_logs/train_mnist/Session_00/run_03\n",
      "x placeholder:  Tensor(\"Placeholder:0\", shape=(100, 784), dtype=float32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (100, 784) for Tensor 'Placeholder_1:0', which has shape '(100,)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-d8af0fd933a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-cd9729091676>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mlogs_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_logs_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Session_00\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-ff387952be2f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(x_train, y_train, batch_size, input_size, num_epochs, writer)\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_summ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msumm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx_ph\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_ph\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m                 \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_summ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatches_per_epoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m                 \u001b[0;34m'which has shape %r'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1104\u001b[0;31m                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m   1105\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (100, 784) for Tensor 'Placeholder_1:0', which has shape '(100,)'"
     ]
    }
   ],
   "source": [
    "run_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
