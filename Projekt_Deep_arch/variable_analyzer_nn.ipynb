{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxi/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from observations import mnist\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist():\n",
    "    \"\"\" loads data from mnist function and converts to numpy arrays\"\"\"\n",
    "\n",
    "    (train_im, train_lab), (test_im, test_lab) = mnist('data/mnist')\n",
    "    train_im_mean = np.mean(train_im, 0)\n",
    "    train_im_std = np.std(train_im, 0)\n",
    "\n",
    "    std_eps = 1e-7\n",
    "    train_im = np.array((train_im - train_im_mean) / (train_im_std + std_eps))\n",
    "    test_im = np.array((test_im - train_im_mean) / (train_im_std + std_eps))\n",
    "\n",
    "    #convert to dictionary\n",
    "    data = {}\n",
    "    data['x_train'] = train_im\n",
    "    data['y_train'] = np.array(train_lab)\n",
    "    data['x_test'] = test_im\n",
    "    data['y_test'] = np.array(test_lab)\n",
    "\n",
    "    return data\n",
    "\n",
    "#expected = y_test\n",
    "#predicted = y_test #Network output\n",
    "\n",
    "#print(\"Classification report:\")\n",
    "#print(classification_report(expected, predicted))\n",
    "#print(\"Confusion matrix:\")\n",
    "#print(confusion_matrix(expected, predicted))\n",
    "\n",
    "def get_logs_path(session):\n",
    "    \"\"\" Returns path where log is going to be saved.\n",
    "    @Input session is the parent folder for the runs\n",
    "    So e.g. (Session1 -> run_00, run_01, run_02 ),\n",
    "    (Session2 -> run_00, run_01)\"\"\"\n",
    "    curr_dir = os.getcwd()\n",
    "    logs_path = curr_dir + '/tensorflow_logs/train_mnist/' + session\n",
    "    if not os.path.exists(logs_path):\n",
    "        os.makedirs(logs_path)\n",
    "    previous_runs = os.listdir(logs_path)\n",
    "    new_path = logs_path + \"/run_{:02d}\".format(len(previous_runs))\n",
    "    if not os.path.exists(new_path):\n",
    "        os.makedirs(new_path)\n",
    "        print(\"Saving log at: \", new_path)\n",
    "        return new_path\n",
    "    raise Exception(\"Path is already existing: \", new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Ziel ist eine nn struktur die sich durch tweaken von eins zwei parametern komplett verändern lässt\n",
    "dabei sollte die depth variable sein, so wie wie viele hidden units ich habe\n",
    "wie die variablen aufgesetzt werden sollte auch per input regulierbar sein\n",
    "Maybe with dropout etc.\"\"\"\n",
    "\n",
    "def forward(input_samples, num_hidden=2, output_units=[20,10,10,10]):\n",
    "    \"\"\" this is the whole network structure, returns the last layer after computing\n",
    "    every other layer\"\"\"\n",
    "    \n",
    "    #assert that inputs are vaguely corretct:\n",
    "    assert len(output_units) == num_hidden + 2,\\\n",
    "    \"output_units length: {}, while only {} num_hidden\".format(len(output_units),  num_hidden)\n",
    "    \n",
    "    #reshape input data\n",
    "    input_layer = input_samples\n",
    "    \n",
    "    #input layer\n",
    "    first_layer = tf.layers.dense(inputs=input_layer, units=output_units[0], name=\"Input_layer\")\n",
    "    \n",
    "    layers = {\n",
    "        \"l0\": first_layer \n",
    "    }\n",
    "    \n",
    "    #some deep layers num_hidden iterations, just shifted one forward\n",
    "    for i in range(1,num_hidden+1):\n",
    "        prev_layer = layers[\"l\"+str(i-1)]\n",
    "        layers[\"l\"+str(i)] = tf.layers.dense(inputs=prev_layer, \n",
    "                                             units=output_units[i], name=\"Hidden_layer_\"+str(i))\n",
    "        \n",
    "    \n",
    "    #output layer\n",
    "    out_layer = tf.layers.dense(inputs=layers[\"l\"+str(num_hidden)], units=output_units[-1], name=\"Output_Layer\")\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x_train, y_train, batch_size, input_size, num_epochs, writer):\n",
    "    \"\"\" trains the network defined in forward \"\"\"\n",
    "    \n",
    "    x_ph = tf.placeholder(tf.float32, [batch_size, input_size], name=\"x_ph\")\n",
    "    y_ph = tf.placeholder(tf.int32, [batch_size], name=\"y_ph\")\n",
    "    \n",
    "    print(\"x placeholder: \", x_ph)\n",
    "    prediction = forward(x_ph)\n",
    "    \n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels= y_ph, logits=prediction)\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    train_op = optimizer.minimize(loss)\n",
    "    \n",
    "    tf.summary.scalar(\"loss\",loss)\n",
    "    summ = tf.summary.merge_all()\n",
    "    \n",
    "    batches_per_epoch = x_train.shape[0] // batch_size\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            for batch in range(batches_per_epoch):\n",
    "                x_batch = x_train[batch * batch_size : (batch+1) * batch_size]\n",
    "                y_batch = y_train[batch * batch_size : (batch+1) * batch_size]\n",
    "                \n",
    "                print(\"shape of input\", y_batch.shape)\n",
    "                \n",
    "                _, cur_loss, cur_summ = sess.run([train_op, loss, summ], feed_dict={x_ph: x_batch, y_ph: y_batch})\n",
    "                writer.add_summary(cur_summ, epoch*batches_per_epoch + batch)\n",
    "                print(cur_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model():\n",
    "    x_train, y_train, x_test, y_test = get_mnist(1000,300)\n",
    "    logs_path = get_logs_path(\"Session_00\")\n",
    "    writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "    train(x_train=x_train, y_train=y_train, batch_size=100, input_size=784, num_epochs=5, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[1. 7. 9. 2. 1. 1. 5. 9. 9. 9. 3. 6. 9. 2. 1. 6. 1. 0. 7. 0. 8. 0. 8. 2.\n 7. 1. 6. 5. 0. 7. 3. 2. 9. 1. 5. 7. 9. 3. 0. 0. 8. 5. 2. 3. 7. 2. 7. 3.\n 6. 3. 9. 5. 6. 0. 8. 8. 7. 7. 8. 2. 7. 0. 2. 6. 4. 6. 2. 1. 3. 9. 0. 4.\n 8. 4. 3. 9. 5. 9. 0. 3. 2. 4. 5. 7. 2. 9. 9. 8. 3. 2. 5. 4. 3. 4. 0. 2.\n 9. 9. 7. 4. 8. 2. 6. 9. 4. 3. 7. 1. 8. 0. 0. 8. 2. 5. 8. 4. 5. 7. 6. 1.\n 5. 9. 1. 4. 5. 4. 9. 9. 6. 8. 7. 8. 6. 3. 6. 0. 7. 5. 1. 3. 7. 1. 3. 4.\n 7. 8. 6. 7. 8. 2. 0. 2. 7. 6. 2. 9. 5. 9. 1. 3. 5. 2. 2. 4. 8. 1. 6. 3.\n 0. 4. 3. 7. 8. 6. 9. 0. 6. 9. 5. 4. 2. 5. 6. 5. 0. 3. 1. 1. 5. 3. 6. 3.\n 4. 0. 1. 5. 8. 5. 6. 4. 9. 3. 3. 3. 1. 4. 3. 0. 8. 9. 0. 3. 7. 5. 9. 7.\n 2. 5. 7. 9. 2. 6. 6. 3. 9. 7. 6. 1. 9. 7. 0. 7. 4. 9. 8. 2. 2. 5. 9. 6.\n 2. 6. 0. 1. 1. 8. 9. 2. 9. 2. 2. 2. 3. 7. 0. 9. 5. 9. 4. 7. 9. 3. 3. 9.\n 8. 4. 5. 4. 7. 6. 3. 9. 1. 0. 6. 3. 1. 3. 0. 2. 4. 1. 5. 7. 4. 4. 7. 1.\n 4. 2. 7. 9. 9. 9. 9. 7. 0. 2. 1. 8. 6. 7. 2. 9. 1. 2. 1. 1. 9. 2. 7. 7.\n 2. 8. 1. 2. 0. 0. 6. 8. 4. 0. 6. 9. 2. 0. 0. 0. 1. 3. 9. 8. 2. 7. 1. 4.\n 9. 9. 3. 1. 1. 8. 3. 3. 3. 6. 9. 2. 0. 1. 5. 3. 6. 9. 3. 2. 3. 8. 8. 8.\n 2. 4. 3. 6. 1. 0. 7. 1. 5. 2. 2. 8. 3. 5. 8. 8. 3. 2. 1. 9. 1. 7. 9. 8.\n 6. 0. 0. 3. 0. 2. 8. 4. 4. 8. 9. 8. 9. 7. 4. 5. 9. 6. 9. 4. 7. 2. 7. 7.\n 0. 0. 9. 1. 7. 4. 4. 9. 6. 6. 6. 1. 2. 3. 7. 0. 8. 1. 2. 5. 8. 9. 1. 7.\n 9. 4. 0. 3. 0. 6. 3. 3. 2. 7. 2. 6. 3. 3. 8. 5. 1. 4. 9. 0. 0. 4. 3. 7.\n 2. 8. 2. 4. 9. 5. 8. 3. 5. 1. 2. 6. 6. 5. 4. 4. 2. 3. 4. 0. 1. 3. 6. 9.\n 6. 3. 9. 7. 0. 2. 2. 4. 5. 5. 0. 3. 6. 1. 6. 4. 2. 6. 3. 3. 3. 2. 8. 4.\n 4. 3. 6. 9. 7. 6. 5. 1. 4. 5. 9. 9. 9. 4. 2. 6. 5. 1. 3. 4. 6. 1. 1. 1.\n 3. 3. 2. 9. 7. 1. 5. 4. 7. 6. 3. 6. 2. 7. 6. 1. 6. 8. 9. 4. 1. 0. 1. 7.\n 8. 0. 6. 5. 5. 3. 8. 7. 3. 9. 8. 7. 8. 2. 4. 0. 1. 3. 2. 8. 8. 7. 7. 0.\n 5. 5. 5. 7. 7. 3. 6. 9. 6. 5. 4. 0. 8. 0. 6. 1. 2. 6. 4. 5. 7. 2. 0. 9.\n 0. 5. 3. 6. 3. 2. 0. 4. 3. 4. 3. 6. 1. 9. 3. 0. 6. 5. 1. 2. 5. 3. 8. 9.\n 4. 1. 6. 1. 3. 8. 8. 9. 1. 7. 7. 5. 3. 1. 2. 2. 3. 9. 2. 1. 6. 7. 1. 6.\n 1. 8. 1. 6. 4. 0. 5. 8. 3. 5. 7. 8. 5. 8. 4. 3. 1. 1. 5. 5. 6. 8. 1. 4.\n 5. 6. 2. 5. 5. 4. 4. 0. 2. 9. 5. 8. 9. 7. 6. 6. 5. 7. 4. 6. 4. 7. 7. 6.\n 4. 6. 5. 3. 6. 6. 1. 0. 7. 0. 7. 8. 0. 4. 1. 4. 0. 3. 1. 2. 6. 5. 9. 3.\n 1. 5. 0. 1. 6. 4. 9. 3. 0. 4. 7. 2. 4. 1. 2. 6. 9. 3. 6. 5. 0. 4. 1. 5.\n 1. 2. 2. 1. 2. 9. 5. 5. 1. 2. 8. 6. 2. 0. 8. 6. 7. 1. 2. 3. 2. 1. 5. 2.\n 4. 1. 0. 4. 0. 0. 4. 6. 3. 2. 6. 9. 6. 2. 1. 2. 8. 4. 9. 4. 7. 1. 5. 8.\n 1. 9. 7. 9. 5. 5. 5. 6. 6. 1. 9. 7. 2. 5. 8. 8. 0. 8. 9. 3. 6. 1. 5. 7.\n 4. 2. 7. 7. 2. 1. 2. 6. 0. 7. 6. 0. 9. 8. 2. 8. 3. 0. 4. 9. 4. 4. 5. 7.\n 1. 3. 1. 0. 4. 8. 3. 4. 0. 2. 6. 2. 8. 3. 3. 2. 4. 4. 1. 1. 7. 0. 9. 2.\n 9. 1. 7. 7. 9. 3. 7. 1. 1. 9. 7. 5. 6. 8. 7. 6. 4. 7. 6. 5. 1. 0. 8. 8.\n 7. 5. 9. 2. 1. 4. 6. 8. 9. 0. 3. 3. 8. 1. 7. 9. 4. 8. 2. 5. 8. 2. 8. 3.\n 8. 2. 1. 2. 1. 1. 4. 9. 9. 5. 3. 0. 5. 5. 2. 1. 6. 3. 8. 7. 0. 3. 7. 4.\n 1. 8. 4. 8. 6. 5. 9. 9. 5. 9. 7. 7. 0. 6. 2. 9. 6. 3. 1. 4. 1. 2. 0. 7.\n 5. 2. 1. 6. 5. 9. 7. 0. 4. 8. 3. 1. 9. 2. 8. 0. 0. 7. 2. 7. 8. 2. 5. 1.\n 3. 2. 4. 5. 8. 5. 7. 5. 3. 4. 6. 4. 2. 6. 2. 1.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-d8af0fd933a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-cd9729091676>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_mnist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mlogs_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_logs_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Session_00\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-e6fadce9b30b>\u001b[0m in \u001b[0;36mget_mnist\u001b[0;34m(train_size, test_size)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X, y, copy)\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m         X = check_array(X, accept_sparse='csr', copy=copy, warn_on_dtype=True,\n\u001b[0;32m--> 681\u001b[0;31m                         estimator=self, dtype=FLOAT_DTYPES)\n\u001b[0m\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    439\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m                     \u001b[0;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[1;32m    442\u001b[0m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0;31m# To ensure that array flags are maintained\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[1. 7. 9. 2. 1. 1. 5. 9. 9. 9. 3. 6. 9. 2. 1. 6. 1. 0. 7. 0. 8. 0. 8. 2.\n 7. 1. 6. 5. 0. 7. 3. 2. 9. 1. 5. 7. 9. 3. 0. 0. 8. 5. 2. 3. 7. 2. 7. 3.\n 6. 3. 9. 5. 6. 0. 8. 8. 7. 7. 8. 2. 7. 0. 2. 6. 4. 6. 2. 1. 3. 9. 0. 4.\n 8. 4. 3. 9. 5. 9. 0. 3. 2. 4. 5. 7. 2. 9. 9. 8. 3. 2. 5. 4. 3. 4. 0. 2.\n 9. 9. 7. 4. 8. 2. 6. 9. 4. 3. 7. 1. 8. 0. 0. 8. 2. 5. 8. 4. 5. 7. 6. 1.\n 5. 9. 1. 4. 5. 4. 9. 9. 6. 8. 7. 8. 6. 3. 6. 0. 7. 5. 1. 3. 7. 1. 3. 4.\n 7. 8. 6. 7. 8. 2. 0. 2. 7. 6. 2. 9. 5. 9. 1. 3. 5. 2. 2. 4. 8. 1. 6. 3.\n 0. 4. 3. 7. 8. 6. 9. 0. 6. 9. 5. 4. 2. 5. 6. 5. 0. 3. 1. 1. 5. 3. 6. 3.\n 4. 0. 1. 5. 8. 5. 6. 4. 9. 3. 3. 3. 1. 4. 3. 0. 8. 9. 0. 3. 7. 5. 9. 7.\n 2. 5. 7. 9. 2. 6. 6. 3. 9. 7. 6. 1. 9. 7. 0. 7. 4. 9. 8. 2. 2. 5. 9. 6.\n 2. 6. 0. 1. 1. 8. 9. 2. 9. 2. 2. 2. 3. 7. 0. 9. 5. 9. 4. 7. 9. 3. 3. 9.\n 8. 4. 5. 4. 7. 6. 3. 9. 1. 0. 6. 3. 1. 3. 0. 2. 4. 1. 5. 7. 4. 4. 7. 1.\n 4. 2. 7. 9. 9. 9. 9. 7. 0. 2. 1. 8. 6. 7. 2. 9. 1. 2. 1. 1. 9. 2. 7. 7.\n 2. 8. 1. 2. 0. 0. 6. 8. 4. 0. 6. 9. 2. 0. 0. 0. 1. 3. 9. 8. 2. 7. 1. 4.\n 9. 9. 3. 1. 1. 8. 3. 3. 3. 6. 9. 2. 0. 1. 5. 3. 6. 9. 3. 2. 3. 8. 8. 8.\n 2. 4. 3. 6. 1. 0. 7. 1. 5. 2. 2. 8. 3. 5. 8. 8. 3. 2. 1. 9. 1. 7. 9. 8.\n 6. 0. 0. 3. 0. 2. 8. 4. 4. 8. 9. 8. 9. 7. 4. 5. 9. 6. 9. 4. 7. 2. 7. 7.\n 0. 0. 9. 1. 7. 4. 4. 9. 6. 6. 6. 1. 2. 3. 7. 0. 8. 1. 2. 5. 8. 9. 1. 7.\n 9. 4. 0. 3. 0. 6. 3. 3. 2. 7. 2. 6. 3. 3. 8. 5. 1. 4. 9. 0. 0. 4. 3. 7.\n 2. 8. 2. 4. 9. 5. 8. 3. 5. 1. 2. 6. 6. 5. 4. 4. 2. 3. 4. 0. 1. 3. 6. 9.\n 6. 3. 9. 7. 0. 2. 2. 4. 5. 5. 0. 3. 6. 1. 6. 4. 2. 6. 3. 3. 3. 2. 8. 4.\n 4. 3. 6. 9. 7. 6. 5. 1. 4. 5. 9. 9. 9. 4. 2. 6. 5. 1. 3. 4. 6. 1. 1. 1.\n 3. 3. 2. 9. 7. 1. 5. 4. 7. 6. 3. 6. 2. 7. 6. 1. 6. 8. 9. 4. 1. 0. 1. 7.\n 8. 0. 6. 5. 5. 3. 8. 7. 3. 9. 8. 7. 8. 2. 4. 0. 1. 3. 2. 8. 8. 7. 7. 0.\n 5. 5. 5. 7. 7. 3. 6. 9. 6. 5. 4. 0. 8. 0. 6. 1. 2. 6. 4. 5. 7. 2. 0. 9.\n 0. 5. 3. 6. 3. 2. 0. 4. 3. 4. 3. 6. 1. 9. 3. 0. 6. 5. 1. 2. 5. 3. 8. 9.\n 4. 1. 6. 1. 3. 8. 8. 9. 1. 7. 7. 5. 3. 1. 2. 2. 3. 9. 2. 1. 6. 7. 1. 6.\n 1. 8. 1. 6. 4. 0. 5. 8. 3. 5. 7. 8. 5. 8. 4. 3. 1. 1. 5. 5. 6. 8. 1. 4.\n 5. 6. 2. 5. 5. 4. 4. 0. 2. 9. 5. 8. 9. 7. 6. 6. 5. 7. 4. 6. 4. 7. 7. 6.\n 4. 6. 5. 3. 6. 6. 1. 0. 7. 0. 7. 8. 0. 4. 1. 4. 0. 3. 1. 2. 6. 5. 9. 3.\n 1. 5. 0. 1. 6. 4. 9. 3. 0. 4. 7. 2. 4. 1. 2. 6. 9. 3. 6. 5. 0. 4. 1. 5.\n 1. 2. 2. 1. 2. 9. 5. 5. 1. 2. 8. 6. 2. 0. 8. 6. 7. 1. 2. 3. 2. 1. 5. 2.\n 4. 1. 0. 4. 0. 0. 4. 6. 3. 2. 6. 9. 6. 2. 1. 2. 8. 4. 9. 4. 7. 1. 5. 8.\n 1. 9. 7. 9. 5. 5. 5. 6. 6. 1. 9. 7. 2. 5. 8. 8. 0. 8. 9. 3. 6. 1. 5. 7.\n 4. 2. 7. 7. 2. 1. 2. 6. 0. 7. 6. 0. 9. 8. 2. 8. 3. 0. 4. 9. 4. 4. 5. 7.\n 1. 3. 1. 0. 4. 8. 3. 4. 0. 2. 6. 2. 8. 3. 3. 2. 4. 4. 1. 1. 7. 0. 9. 2.\n 9. 1. 7. 7. 9. 3. 7. 1. 1. 9. 7. 5. 6. 8. 7. 6. 4. 7. 6. 5. 1. 0. 8. 8.\n 7. 5. 9. 2. 1. 4. 6. 8. 9. 0. 3. 3. 8. 1. 7. 9. 4. 8. 2. 5. 8. 2. 8. 3.\n 8. 2. 1. 2. 1. 1. 4. 9. 9. 5. 3. 0. 5. 5. 2. 1. 6. 3. 8. 7. 0. 3. 7. 4.\n 1. 8. 4. 8. 6. 5. 9. 9. 5. 9. 7. 7. 0. 6. 2. 9. 6. 3. 1. 4. 1. 2. 0. 7.\n 5. 2. 1. 6. 5. 9. 7. 0. 4. 8. 3. 1. 9. 2. 8. 0. 0. 7. 2. 7. 8. 2. 5. 1.\n 3. 2. 4. 5. 8. 5. 7. 5. 3. 4. 6. 4. 2. 6. 2. 1.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "run_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
